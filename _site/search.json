[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SINAPE",
    "section": "",
    "text": "Site com conteúdos do curso de Deep Learning ministrado no SINAPE 2022."
  },
  {
    "objectID": "aula1.html",
    "href": "aula1.html",
    "title": "Aula 1",
    "section": "",
    "text": "# Ler dados\n\ncol_names <- janitor::make_clean_names(c(\"Clump Thickness\",\n\"Uniformity of Cell Size\",\n\"Uniformity of Cell Shape\",\n\"Marginal Adhesion\",\n\"Single Epithelial Cell Size\",\n\"Bare Nuclei\",\n\"Bland Chromatin\",\n\"Normal Nucleoli\",\n\"Mitoses\",\n\"Class\"\n))\n\ncol_names <- c(\n  \"id\", \"diag\",\n  paste0(col_names, \"_mean\"),\n  paste0(col_names, \"_se\"),\n  paste0(col_names, \"_worst\")\n)\n\ndata <- readr::read_csv(\"dados/wdbc.data\",\n                col_names = col_names, na = \"?\")\n\n# Definir o modelo:\n\nlibrary(keras)\nlibrary(tensorflow)\n\ninput <- layer_input(shape = shape(30))\noutput <- input %>%\n  layer_dense(units = 5, activation = \"relu\") %>%\n  layer_dense(units = 1)\nmodel <- keras_model(input, output)\n\n\nmodel %>% compile(\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  optimizer = optimizer_adam(),\n  metrics = \"accuracy\"\n)\n\n# Ajustar o modelo\n\nmodel %>%\n  fit(\n    x = as.matrix(data[-c(1,2)]),\n    y = as.numeric(data$diag == \"M\"),\n    batch_size = 10,\n    epochs = 100,\n    validation_split = 0.2\n  )\n\nget_weights(model)\npredict(model, as.matrix(data[-c(1,2)]))"
  },
  {
    "objectID": "aula1.html#exemplo-2",
    "href": "aula1.html#exemplo-2",
    "title": "Aula 1",
    "section": "Exemplo 2",
    "text": "Exemplo 2\n\nArquivo p/ rodar\n\n\n\nlibrary(tfruns)\n\nruns <- list()\n\nfor (i in 1:5) {\n  runs[[i]] <- tuning_run(\n    \"tune.R\",\n    flags = list(\n      hidden_layer_1 = c(5, 10, 15),\n      hidden_layer_2 = c(0, 5, 10, 15)\n    ),\n    confirm = FALSE\n  )\n}\n\nlibrary(tidyverse)\n\nruns %>%\n  bind_rows() %>%\n  group_by(flag_hidden_layer_1, flag_hidden_layer_2) %>%\n  summarise(\n    mean = mean(metric_val_accuracy),\n    sd = sd(metric_val_accuracy)\n  ) %>%\n  arrange(desc(mean))\n\n\n\n\n\nArquivo espec\n\n\n\n# Ler dados\n\ncol_names <- janitor::make_clean_names(c(\"Clump Thickness\",\n                                         \"Uniformity of Cell Size\",\n                                         \"Uniformity of Cell Shape\",\n                                         \"Marginal Adhesion\",\n                                         \"Single Epithelial Cell Size\",\n                                         \"Bare Nuclei\",\n                                         \"Bland Chromatin\",\n                                         \"Normal Nucleoli\",\n                                         \"Mitoses\",\n                                         \"Class\"\n))\n\ncol_names <- c(\n  \"id\", \"diag\",\n  paste0(col_names, \"_mean\"),\n  paste0(col_names, \"_se\"),\n  paste0(col_names, \"_worst\")\n)\n\ndata <- readr::read_csv(\"dados/wdbc.data\",\n                        col_names = col_names, na = \"?\")\n\n\n# Variáveis:\n\nlibrary(tfruns)\n\nFLAGS <- flags(\n  flag_integer('hidden_layer_1', 5, 'Size of the first hidden layer'),\n  flag_integer(\"hidden_layer_2\", 0, \"Size of the second hidden layer\")\n)\n\n# Definir o modelo:\n\nlibrary(keras)\nlibrary(tensorflow)\n\ninput <- layer_input(shape = shape(30))\noutput <- input %>%\n  layer_dense(units = FLAGS$hidden_layer_1, activation = \"relu\")\n\nif (FLAGS$hidden_layer_2 > 0) {\n  output <- output %>%\n    layer_dense(units = FLAGS$hidden_layer_2, activation = \"relu\")\n}\n\noutput <- output %>%\n  layer_dense(units = 1)\n\nmodel <- keras_model(input, output)\n\nmodel %>% compile(\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  optimizer = optimizer_adam(),\n  metrics = \"accuracy\"\n)\n\n# Ajustar o modelo\n\nmodel %>%\n  fit(\n    x = as.matrix(data[-c(1,2)]),\n    y = as.numeric(data$diag == \"M\"),\n    batch_size = 10,\n    epochs = 100,\n    validation_split = 0.2,\n    callbacks = list(\n      callback_early_stopping(patience = 5)\n    )\n  )"
  },
  {
    "objectID": "aula1.html#exemplo-3",
    "href": "aula1.html#exemplo-3",
    "title": "Aula 1",
    "section": "Exemplo 3",
    "text": "Exemplo 3\n\n\n\n# Carregando os dados\n\nlibrary(keras)\nlibrary(tensorflow)\n\narquivos <- fs::dir_ls(\"dados/images/\", glob = \"*.jpg\")\nclasses <- arquivos %>%\n  fs::path_file() %>%\n  stringr::str_extract(\"(.*)_\") %>%\n  stringr::str_sub(end = -2)\n\nall_class <- unique(classes)\nclasses_int <- match(classes, all_class) - 1L\n\nlibrary(tfdatasets)\n\nmake_dataset <- function(arquivos, classes_int) {\n  tensor_slices_dataset(list(arq = arquivos, classe = classes_int)) %>%\n    dataset_map(function(x) {\n      img <- x$arq %>%\n        tf$io$read_file() %>%\n        tf$image$decode_jpeg(channels = 3) %>%\n        tf$image$resize(c(32L, 32L)) %>%\n        tf$image$convert_image_dtype(tf$float32)\n      list(img, x$classe)\n    }, num_parallel_calls = tf$data$AUTOTUNE) %>%\n    dataset_batch(32) %>%\n    dataset_prefetch(tf$data$AUTOTUNE)\n}\n\n\nid_train <- sample.int(length(arquivos), 0.8*length(arquivos))\n\ntrain_dataset <- make_dataset(\n  arquivos = arquivos[id_train],\n  classes_int = classes_int[id_train]\n)\n\nvalid_dataset <- make_dataset(\n  arquivos = arquivos[-id_train],\n  classes_int = classes_int[-id_train]\n)\n\ncoro::collect(train_dataset, 2)[[1]][[1]][4,,,] %>%\n  as.array() %>%\n  as.raster(max = 255) %>%\n  plot()\n\n# Construir o modelo\n\ninput <- layer_input(shape = shape(32, 32, 3))\noutput <- input %>%\n  layer_rescaling(scale = 1/255) %>%\n\n  layer_conv_2d(\n    filter = 16, kernel_size = c(3,3), padding = \"same\",\n    input_shape = c(32, 32, 3)\n  ) %>%\n  layer_activation_leaky_relu(0.1) %>%\n\n  # Second hidden layer\n  layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%\n  layer_activation_leaky_relu(0.1) %>%\n\n  # Use max pooling\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\n  layer_dropout(0.25) %>%\n\n  # 2 additional hidden 2D convolutional layers\n  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = \"same\") %>%\n  layer_activation_leaky_relu(0.1) %>%\n  layer_conv_2d(filter = 64, kernel_size = c(3,3)) %>%\n  layer_activation_leaky_relu(0.1) %>%\n\n  # Use max pooling once more\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\n  layer_dropout(0.25) %>%\n\n  # Flatten max filtered output into feature vector\n  # and feed into dense layer\n  layer_flatten() %>%\n  layer_dense(256) %>%\n  layer_activation_leaky_relu(0.1) %>%\n  layer_dropout(0.5) %>%\n\n  # Outputs from dense layer are projected onto 10 unit output layer\n  layer_dense(37)\n\nmodel <- keras_model(input, output)\nmodel %>%\n  compile(\n    loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\n\nmodel %>%\n  fit(\n    train_dataset,\n    validation_data = valid_dataset\n  )"
  },
  {
    "objectID": "aula1.html#exemplo-4",
    "href": "aula1.html#exemplo-4",
    "title": "Aula 1",
    "section": "Exemplo 4",
    "text": "Exemplo 4\n\n\n\n# Carregando os dados\n\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(tfdatasets)\n\narquivos <- fs::dir_ls(\"dados/images/\", glob = \"*.jpg\")\ntrimaps <- fs::path(\n  \"dados/annotations/trimaps/\",\n  fs::path_file(arquivos)\n)\nfs::path_ext(trimaps) <- \"png\"\n\nimg_size <- c(32L, 32L)\n\nread_trimap <- function(trimap) {\n  tr <- trimap %>%\n    tf$io$read_file() %>%\n    tf$image$decode_png() %>%\n    tf$image$resize(img_size)\n  tr - 1L\n}\n\ndisplay_target <- function(target_array) {\n  normalized_array <- (target_array) * 127\n  normalized_array <- tf$image$grayscale_to_rgb(as_tensor(normalized_array))\n  normalized_array <- as.raster(as.array(normalized_array), max = 255)\n  plot(normalized_array)\n}\n\ndisplay_target(as.array(read_trimap(trimaps[1])))\n\nread_img <- function(arq) {\n  arq %>%\n    tf$io$read_file() %>%\n    tf$image$decode_jpeg(channels = 3) %>%\n    tf$image$resize(img_size) %>%\n    tf$image$convert_image_dtype(tf$float32)\n}\n\nread_img(arquivos[2]) %>%\n  as.array() %>%\n  as.raster(max = 255) %>%\n  plot()\n\n\nmake_dataset <- function(arquivos, trimaps) {\n  list(arquivos = arquivos, trimaps = trimaps) %>%\n    tensor_slices_dataset() %>%\n    dataset_map(function(x) {\n      list(\n        read_img(x$arquivos),\n        read_trimap(x$trimaps)\n      )\n    }, num_parallel_calls = tf$data$AUTOTUNE) %>%\n    dataset_batch(32) %>%\n    dataset_prefetch(tf$data$AUTOTUNE)\n}\n\nid_train <- sample.int(length(arquivos), 0.8*length(arquivos))\n\ntrain_dataset <- make_dataset(\n  arquivos = arquivos[id_train],\n  trimaps = trimaps[id_train]\n)\n\nvalid_dataset <- make_dataset(\n  arquivos = arquivos[-id_train],\n  trimaps = trimaps[-id_train]\n)\n\ncoro::collect(train_dataset, 2)[[1]][[1]][4,,,] %>%\n  as.array() %>%\n  as.raster(max = 255) %>%\n  plot()\n\n# Construir o modelo\n\nget_model <- function(img_size, num_classes) {\n  input <- layer_input(shape = c(img_size, 3))\n  output <- input %>%\n    layer_rescaling(scale = 1/255) %>%\n    layer_conv_2d(filters = 64, kernel_size = 3, strides = 2, activation = \"relu\",\n                  padding = \"same\") %>%\n    layer_conv_2d(filters = 64, kernel_size = 3, activation = \"relu\",\n                  padding = \"same\") %>%\n    layer_conv_2d(filters = 128, kernel_size = 3, strides = 2, activation = \"relu\",\n                  padding = \"same\") %>%\n    layer_conv_2d(filters = 128, kernel_size = 3, activation = \"relu\",\n                  padding = \"same\") %>%\n    layer_conv_2d(filters = 256, kernel_size = 3, strides = 2, activation = \"relu\",\n                  padding = \"same\") %>%\n    layer_conv_2d(filters = 256, kernel_size = 3, activation = \"relu\",\n                  padding = \"same\") %>%\n\n    layer_conv_2d_transpose(filters = 256, kernel_size = 3, activation = \"relu\",\n                            padding = \"same\") %>%\n    layer_conv_2d_transpose(filters = 256, kernel_size = 3, activation = \"relu\",\n                            padding = \"same\", strides = 2) %>%\n    layer_conv_2d_transpose(filters = 128, kernel_size = 3, activation = \"relu\",\n                            padding = \"same\") %>%\n    layer_conv_2d_transpose(filters = 128, kernel_size = 3, activation = \"relu\",\n                            padding = \"same\", strides = 2) %>%\n    layer_conv_2d_transpose(filters = 64, kernel_size = 3, activation = \"relu\",\n                            padding = \"same\") %>%\n    layer_conv_2d_transpose(filters = 64, kernel_size = 3, activation = \"relu\",\n                            padding = \"same\", strides = 2) %>%\n\n    layer_conv_2d(num_classes, 3, activation=\"softmax\", padding=\"same\")\n\n\n  keras_model(input, output)\n}\n\nmodel <- get_model(img_size=img_size, num_classes=3)\nmodel\n\nmodel %>%\n  compile(\n    loss = \"sparse_categorical_crossentropy\",\n    optimizer = \"rmsprop\"\n  )\n\n\nmodel %>%\n  fit(\n    train_dataset,\n    validation_data = valid_dataset\n  )"
  },
  {
    "objectID": "aula1.html#exemplo-1",
    "href": "aula1.html#exemplo-1",
    "title": "Aula 1",
    "section": "Exemplo 1",
    "text": "Exemplo 1\n\n\n\n# Ler dados\n\ncol_names <- janitor::make_clean_names(c(\"Clump Thickness\",\n\"Uniformity of Cell Size\",\n\"Uniformity of Cell Shape\",\n\"Marginal Adhesion\",\n\"Single Epithelial Cell Size\",\n\"Bare Nuclei\",\n\"Bland Chromatin\",\n\"Normal Nucleoli\",\n\"Mitoses\",\n\"Class\"\n))\n\ncol_names <- c(\n  \"id\", \"diag\",\n  paste0(col_names, \"_mean\"),\n  paste0(col_names, \"_se\"),\n  paste0(col_names, \"_worst\")\n)\n\ndata <- readr::read_csv(\"dados/wdbc.data\",\n                col_names = col_names, na = \"?\")\n\n# Definir o modelo:\n\nlibrary(keras)\nlibrary(tensorflow)\n\ninput <- layer_input(shape = shape(30))\noutput <- input %>%\n  layer_dense(units = 5, activation = \"relu\") %>%\n  layer_dense(units = 1)\nmodel <- keras_model(input, output)\n\n\nmodel %>% compile(\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  optimizer = optimizer_adam(),\n  metrics = \"accuracy\"\n)\n\n# Ajustar o modelo\n\nmodel %>%\n  fit(\n    x = as.matrix(data[-c(1,2)]),\n    y = as.numeric(data$diag == \"M\"),\n    batch_size = 10,\n    epochs = 100,\n    validation_split = 0.2\n  )\n\nget_weights(model)\npredict(model, as.matrix(data[-c(1,2)]))"
  }
]